{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2:1b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Simple Chain with Retrieval\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Implement a simple RAG chain with ChatOllama, HuggingFaceEmbeddings and Chroma.\n",
    "(Implementieren Sie eine einfache RAG-Kette mit ChatOllama, HuggingFaceEmbeddings und Chroma.)\n",
    "\n",
    "Process:\n",
    "\n",
    "1. Retrieve documents from chroma db based on query\n",
    "   (Rufen Sie Dokumente basierend auf der Abfrage aus der Chroma-Datenbank ab)\n",
    "2. Invoke chain with retrieved documents as input\n",
    "   (Aufrufkette mit abgerufenen Dokumenten als Eingabe)\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load llm model via ollama\n",
    "- load huggingface embedding model (`model_name=\"sentence-transformers/all-mpnet-base-v2\"`)\n",
    "- create chroma db client\n",
    "- create prompt template for summarization\n",
    "- create simple chain with following steps: retrieved documents, prompt, model, output parser\n",
    "- create query and perform similarity search with a query\n",
    "- invoke chain and pass retrieved documents to the chain\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [RAG with Ollama](https://python.langchain.com/v0.2/docs/tutorials/local_rag/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "llm = ChatOllama(model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/TeamNovaBot/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/workspaces/TeamNovaBot/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "embedding_model = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "client = chromadb.HttpClient(\n",
    "    host     = \"localhost\",\n",
    "    port     = 8000,\n",
    "    ssl      = False,\n",
    "    headers  = None,\n",
    "    settings = Settings(allow_reset=True, anonymized_telemetry=False),\n",
    "    tenant   = DEFAULT_TENANT,\n",
    "    database = DEFAULT_DATABASE,\n",
    ")\n",
    "\n",
    "# Create a collection\n",
    "# ADD HERE YOUR CODE\n",
    "collection_name = \"AI_Book\"\n",
    "collection = client.get_or_create_collection(collection_name)\n",
    "\n",
    "# Create chromadb\n",
    "# ADD HERE YOUR CODE\n",
    "#vector_db_from_client = Chroma(persist_directory = \"chroma_db\") erster Versuch\n",
    "vector_db_from_client = Chroma(\n",
    "    client = client,\n",
    "    collection_name = collection_name,\n",
    "    embedding_function = embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convert loaded documents into strings by concatenating their content\n",
    "# and ignoring metadata\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = {\"docs\": format_docs} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Types of Machine Learning Systems\"\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Perform vector search\n",
    "docs = vector_db_from_client.similarity_search(search_query)\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"However, I need to clarify that I don't see any documents retrieved. Could you please provide me with the documents you'd like me to summarize? Additionally, could you please let me know which retrieved docs you're referring to (e.g., a specific text or a set of related texts)?\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Q&A with RAG\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Implement a Q/A retrieval chain with ChatOllama, HuggingFaceEmbeddings and Chroma\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- create RAG-Q/A prompt template\n",
    "- create retriever from vector db client (instead of manually passing in docs, we automatically retrieve them from our vector store based on the user question)\n",
    "- create simple chain with following steps: retriever, formatting retrieved docs, user question, prompt, model, output parser\n",
    "- create question for Q/A retrieval chain\n",
    "- invoke chain and with question\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [RAG with Ollama](https://python.langchain.com/v0.2/docs/tutorials/local_rag/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "rag_prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "retriever = vector_db_from_client.as_retriever()\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "qa_rag_chain = ({\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7fb6ba313450>)\n",
       "           | RunnableLambda(format_docs),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n<context>\\n{context}\\n</context>\\n\\nAnswer the following question:\\n\\n{question}\"))])\n",
       "| ChatOllama(model='llama3.2:1b', _client=<ollama._client.Client object at 0x7fb6ba077610>, _async_client=<ollama._client.AsyncClient object at 0x7fb6ba028190>)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Supervised learning is a type of machine learning where an algorithm is trained on labeled data to make predictions or decisions based on specific examples, with the goal of minimizing the difference between predicted outcomes and actual outcomes. The labeled data provides feedback in the form of correct or incorrect responses, allowing the algorithm to learn patterns and relationships between variables. This process enables supervised learning algorithms to improve their performance over time by adjusting their weights and biases through iterative training.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is supervised learning?\"\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "qa_rag_chain.invoke(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
