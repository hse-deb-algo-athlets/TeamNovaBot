{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Simple Chain with Retrieval\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Implement a simple RAG chain with ChatOllama, HuggingFaceEmbeddings and Chroma. \n",
    "\n",
    "Process: \n",
    "\n",
    "1. Retrieve documents from chroma db based on query\n",
    "2. Invoke chain with retrieved documents as input\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load llm model via ollama\n",
    "- load huggingface embedding model (``model_name=\"sentence-transformers/all-mpnet-base-v2\"``)\n",
    "- create chroma db client\n",
    "- create prompt template for summarization\n",
    "- create simple chain with following steps: retrieved documents, prompt, model, output parser\n",
    "- create query and perform similarity search with a query\n",
    "- invoke chain and pass retrieved documents to the chain\n",
    "\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [RAG with Ollama](https://python.langchain.com/v0.2/docs/tutorials/local_rag/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "llm = ChatOllama(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sschn\\OneDrive\\Documents\\Studium\\3.Semester\\Technische Informatik\\Chatbot\\TeamNovaBot\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "client = chromadb.HttpClient(\n",
    "    host=\"localhost\",\n",
    "    port=8000,\n",
    "    ssl=False,\n",
    "    headers=None,\n",
    "    settings=Settings(allow_reset=True, anonymized_telemetry=False),\n",
    "    tenant=DEFAULT_TENANT,\n",
    "    database=DEFAULT_DATABASE,\n",
    ")\n",
    "\n",
    "# Create a collection\n",
    "# ADD HERE YOUR CODE\n",
    "collection = client.get_or_create_collection(\"collection_name\")\n",
    "\n",
    "# Create chromadb\n",
    "# ADD HERE YOUR CODE\n",
    "vector_db_from_client = Chroma(client=client,collection_name=\"collection_name\",embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convert loaded documents into strings by concatenating their content\n",
    "# and ignoring metadata\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = {\"docs\": format_docs} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'page': 33, 'source': './AI_Book.pdf'}, page_content='Types of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on:\\nWhether or not they are trained with human supervision (supervised, unsuper\\nvised, semisupervised, and Reinforcement Learning)\\nWhether or not they can learn incrementally on the fly (online versus batch\\nlearning)\\nWhether they work by simply comparing new data points to known data points,\\nor instead detect patterns in the training data and build a predictive model, much\\nlike scientists do (instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For\\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net\\nwork model trained using examples of spam and ham; this makes it an online, model-\\nbased, supervised learning system.\\nLets look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training. There are four major categories: supervised\\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn\\ning.\\nSupervised learning\\nIn supervised learning , the training data you feed to the algorithm includes the desired\\nsolutions, called labels  (Figure 1-5 ).\\nFigure 1-5. A labeled training set for supervised learning (e.g., spam classification)\\n8 | Chapter 1: The Machine Learning Landscape'), Document(metadata={'page': 40, 'source': './AI_Book.pdf'}, page_content='Figure 1-12. Reinforcement Learning\\nFor example, many robots implement Reinforcement Learning algorithms to learn\\nhow to walk. DeepMinds AlphaGo program is also a good example of Reinforcement\\nLearning: it made the headlines in May 2017 when it beat the world champion Ke Jie\\nat the game of Go. It learned its winning policy by analyzing millions of games, and\\nthen playing many games against itself. Note that learning was turned off during the\\ngames against the champion; AlphaGo was just applying the policy it had learned.\\nBatch and Online Learning\\nAnother criterion used to classify Machine Learning systems is whether or not the\\nsystem can learn incrementally from a stream of incoming data.\\nBatch learning\\nIn batch learning , the system is incapable of learning incrementally: it must be trained\\nusing all the available data. This will generally take a lot of time and computing\\nresources, so it is typically done offline. First the system is trained, and then it is\\nlaunched into production and runs without learning anymore; it just applies what it\\nhas learned. This is called offline  learning .\\nIf you want a batch learning system to know about new data (such as a new type of\\nspam), you need to train a new version of the system from scratch on the full dataset\\n(not just the new data, but also the old data), then stop the old system and replace it\\nwith the new one.\\nFortunately, the whole process of training, evaluating, and launching a Machine\\nLearning system can be automated fairly easily (as shown in Figure 1-3 ), so even a\\nTypes of Machine Learning Systems | 15'), Document(metadata={'page': 44, 'source': './AI_Book.pdf'}, page_content='Figure 1-15. Instance-based learning\\nModel-based learning\\nAnother way to generalize from a set of examples is to build a model of these exam\\nples, then use that model to make predictions . This is called model-based learning\\n(Figure 1-16 ).\\nFigure 1-16. Model-based learning\\nFor example, suppose you want to know if money makes people happy, so you down\\nload the Better Life Index  data from the OECDs website  as well as stats about GDP\\nper capita from the IMFs website . Then you join the tables and sort by GDP per cap\\nita. Table 1-1  shows an excerpt of what you get.\\nTypes of Machine Learning Systems | 19'), Document(metadata={'page': 28, 'source': './AI_Book.pdf'}, page_content='CHAPTER 1\\nThe Machine Learning Landscape\\nWith Early Release ebooks, you get books in their earliest form\\nthe authors raw and unedited content as he or she writesso you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 1 in the final\\nrelease of the book.\\nWhen most people hear Machine Learning,  they picture a robot: a dependable but\\nler or a deadly Terminator depending on who you ask. But Machine Learning is not\\njust a futuristic fantasy, its already here. In fact, it has been around for decades in\\nsome specialized applications, such as Optical Character Recognition  (OCR). But the\\nfirst ML application that really became mainstream, improving the lives of hundreds\\nof millions of people, took over the world back in the 1990s: it was the spam filter .\\nNot exactly a self-aware Skynet, but it does technically qualify as Machine Learning\\n(it has actually learned so well that you seldom need to flag an email as spam any\\nmore). It was followed by hundreds of ML applications that now quietly power hun\\ndreds of products and features that you use regularly, from better recommendations\\nto voice search.\\nWhere does Machine Learning start and where does it end? What exactly does it\\nmean for a machine to learn  something? If I download a copy of Wikipedia, has my\\ncomputer really learned something? Is it suddenly smarter? In this chapter we will\\nstart by clarifying what Machine Learning is and why you may want to use it.\\nThen, before we set out to explore the Machine Learning continent, we will take a\\nlook at the map and learn about the main regions and the most notable landmarks:\\nsupervised versus unsupervised learning, online versus batch learning, instance-\\nbased versus model-based learning. Then we will look at the workflow of a typical ML\\nproject, discuss the main challenges you may face, and cover how to evaluate and\\nfine-tune a Machine Learning system.\\n3')]\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Types of Machine Learning Systems\"\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Perform vector search\n",
    "docs = vector_db_from_client.similarity_search(search_query)\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main themes in these retrieved docs are:\\n\\n1. **Types of Machine Learning Systems**: The docs categorize machine learning systems based on four criteria: supervision, incremental learning, instance-based vs model-based learning, and online vs batch learning.\\n2. **Supervised/Unsupervised Learning**: Supervised learning involves training with labeled data to predict outcomes, while unsupervised learning involves discovering patterns in unlabeled data.\\n3. **Batch and Online Learning**: Batch learning involves training on a fixed dataset before deploying the model, whereas online learning allows for incremental updates based on new incoming data.\\n4. **Instance-based vs Model-based Learning**: Instance-based learning involves making predictions based on specific examples (instance-based), while model-based learning builds a predictive model from these examples (model-based).\\n5. **The Scope of Machine Learning**: The docs highlight the wide range of applications and systems that use machine learning, including spam filters, OCR, recommendations, and voice search.\\n6. **Clarifying Machine Learning**: The chapter aims to clarify what machine learning means, how it works, and its limitations, setting the stage for further exploration of the field.\\n\\nThese themes provide a foundation for understanding the diversity of machine learning systems and their applications, as well as the key concepts and challenges involved in building and deploying these systems.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Q&A with RAG\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Implement a Q/A retrieval chain with ChatOllama, HuggingFaceEmbeddings and Chroma\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- create RAG-Q/A prompt template\n",
    "- create retriever from vector db client (instead of manually passing in docs, we automatically retrieve them from our vector store based on the user question)\n",
    "- create simple chain with following steps: retriever, formatting retrieved docs, user question, prompt, model, output parser\n",
    "- create question for Q/A retrieval chain\n",
    "- invoke chain and with question\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [RAG with Ollama](https://python.langchain.com/v0.2/docs/tutorials/local_rag/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "rag_prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "retriever = vector_db_from_client.as_retriever()\n",
    "# ADD HERE YOUR CODE\n",
    "qa_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x00000131F859DC10>)\n",
       "           | RunnableLambda(format_docs),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n<context>\\n{context}\\n</context>\\n\\nAnswer the following question:\\n\\n{question}\"))])\n",
       "| ChatOllama(model='llama3.2', _client=<ollama._client.Client object at 0x00000131F8D08150>, _async_client=<ollama._client.AsyncClient object at 0x00000131F26E9FD0>)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Supervised learning is a type of machine learning where the training data includes the desired solutions, called labels. The algorithm learns from this labeled data to make predictions on new, unseen data. In supervised learning, the goal is to learn a mapping between inputs and outputs, such as classifying spam emails as \"spam\" or predicting the price of a car based on features like mileage and age.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is supervised learning?\"\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "qa_rag_chain.invoke(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
