{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Simple vector embedding generation\n",
    "\n",
    "**Objective:**\n",
    "Generate vector embeddings from text data.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load huggingface embedding model (`model_name=\"sentence-transformers/all-mpnet-base-v2\"`)\n",
    "- embed simple text queries\n",
    "\n",
    "How to select the right embedding model: [MTEB - Massive Text Embedding Benchmark](https://huggingface.co/blog/mteb)\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain Chroma](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/TeamNovaBot/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/workspaces/TeamNovaBot/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "embedding_model = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector length: 24\n",
      "This is a \n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test document.\"\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Perform vector search\n",
    "query_vector = text\n",
    "\n",
    "print(f\"Embedding vector length: {len(query_vector)}\")\n",
    "print(query_vector[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Generate embedding vectors with custom dataset\n",
    "\n",
    "**Objective:**\n",
    "Load custom dataset, preprocess it and generate vector embeddings.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load pdf document \"AI_Book.pdf\" via langchain document loader: `PyPDFLoader`\n",
    "- use RecursiveCharacterTextSplitter to split documents into chunks\n",
    "- generate embeddings for single documents\n",
    "\n",
    "**RecursiveCharacterTextSplitter:**\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain PyPDFLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html)\n",
    "- [Langchain RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Aurélien GéronHands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and\n",
      "TensorFlow\n",
      "Concepts, Tools, and Techniques to\n",
      "Build Intelligent SystemsSECOND EDITION\n",
      "Boston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing' metadata={'source': './AI_Book.pdf', 'page': 2}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import re\n",
    "\n",
    "pdf_doc = \"./AI_Book.pdf\"\n",
    "\n",
    "# Create pdf data loader\n",
    "# ADD HERE YOUR CODE\n",
    "loader = PyPDFLoader(\n",
    "    file_path = pdf_doc,\n",
    "    # passwort = \"my-passwort\"\n",
    "    extract_images = False,\n",
    ")\n",
    "\n",
    "# Load and split documents in chunks\n",
    "# ADD HERE YOUR CODE\n",
    "pages_chunked = RecursiveCharacterTextSplitter().split_documents(documents = loader.load())\n",
    "\n",
    "# Function to clean text by removing invalid unicode characters, including surrogate pairs\n",
    "def clean_text(text):\n",
    "    # Remove surrogate pairs\n",
    "    text = re.sub(r'[\\ud800-\\udfff]', '', text)\n",
    "    # Optionally remove non-ASCII characters (depends on your use case)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_and_create_document(chunk):\n",
    "    cleaned_text = clean_text(chunk.page_content)\n",
    "    return Document(page_content = cleaned_text, metadata = chunk.metadata)\n",
    "\n",
    "pages_chunked_cleaned1 = [clean_and_create_document(chunk) for chunk in pages_chunked]\n",
    "pages_chunked_cleaned = [clean_text(chunk.page_content) for chunk in pages_chunked]\n",
    "print(pages_chunked[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='978-1-492-03264-9\n",
      "[LSI]Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
      "by Aurélien Géron\n",
      "Copyright © 2019 Aurélien Géron. All rights reserved.\n",
      "Printed in the United States of America.\n",
      "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
      "also available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\n",
      "sales department: 800-998-9938 or corporate@oreilly.com .\n",
      "Editor:  Nicole Tache\n",
      "Interior Designer:  David FutatoCover Designer:  Karen Montgomery\n",
      "Illustrator:  Rebecca Demarest\n",
      "June 2019:  Second Edition\n",
      "Revision History for the Early Release\n",
      "2018-11-05: First Release\n",
      "2019-01-24: Second Release\n",
      "2019-03-07: Third Release\n",
      "2019-03-29: Fourth Release\n",
      "2019-04-22: Fifth Release\n",
      "See http://oreilly.com/catalog/errata.csp?isbn=9781492032649  for release details.\n",
      "The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and TensorFlow , the cover image, and related trade dress are trademarks of O’Reilly\n",
      "Media, Inc.\n",
      "While the publisher and the author have used good faith efforts to ensure that the information and\n",
      "instructions contained in this work are accurate, the publisher and the author disclaim all responsibility\n",
      "for errors or omissions, including without limitation responsibility for damages resulting from the use of\n",
      "or reliance on this work. Use of the information and instructions contained in this work is at your own\n",
      "risk. If any code samples or other technology this work contains or describes is subject to open source\n",
      "licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\n",
      "thereof complies with such licenses and/or rights.' metadata={'source': './AI_Book.pdf', 'page': 3}\n"
     ]
    }
   ],
   "source": [
    "print(pages_chunked[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 507\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of text chunks: {len(pages_chunked)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Store vector embeddings from pdf document to ChromaDB vector database.\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Store vector embeddings into ChromaDB to store knowledge.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- create chromadb client\n",
    "- create chromadb collection\n",
    "- create langchain chroma db client\n",
    "- store text document chunks and vector embeddings to vector databases\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain How To](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/#initialization-from-client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "client = chromadb.HttpClient(\n",
    "    host     = \"localhost\",\n",
    "    port     = 8000,\n",
    "    ssl      = False,\n",
    "    headers  = None,\n",
    "    settings = Settings(allow_reset = True, anonymized_telemetry = False),\n",
    "    tenant   = DEFAULT_TENANT,\n",
    "    database = DEFAULT_DATABASE,\n",
    ")\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Create a collection\n",
    "collection = client.get_or_create_collection(\"collection_name\")\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Create chromadb\n",
    "vector_db_from_client = Chroma(\n",
    "    client = client,\n",
    "    collection_name = \"collection_name\",\n",
    "    embedding_function = embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f2402326-2047-466e-acc9-8ac69b511b8f',\n",
       " '4f084334-f51a-4f66-bfff-d48171293324',\n",
       " 'e14de60d-5b2f-4ec1-8c53-b9a9f1ac9159',\n",
       " 'c082fec5-1fa6-49b6-8480-241f9ae004ee',\n",
       " '5a1d587f-020a-4083-af6a-40bfdfc93025',\n",
       " '5b8cf9e9-8131-47c8-99bf-4d7cbf2ea976',\n",
       " '34b4c49d-b802-4e1a-90d3-f44a323b7e7d',\n",
       " '9ef13394-b6ad-4ce3-b88e-d20284003ba4',\n",
       " 'b00299e8-5b90-4162-8c68-9636fce05256',\n",
       " '36ebb99a-2cfa-41b7-97ce-e0b518b0e96b',\n",
       " '211bb5f9-37f4-4c9b-a9c5-8b2496f2c8a0',\n",
       " '2f97f893-6db4-4237-aee2-c04f7eae2f98',\n",
       " '6b718c53-2948-4672-8ccc-d7cb0766337a',\n",
       " '5ab563d3-f81d-4c8c-a0c9-098752eadd09',\n",
       " 'f4595396-7539-4ae3-896e-4c79db8df5f9',\n",
       " 'dc477949-0515-4684-8e00-713a26d069c2',\n",
       " 'd842df6f-93c8-4b8e-8a10-615eb572b3b2',\n",
       " 'd65222ff-7f43-46c9-98c7-c0e98e74fd88',\n",
       " '8463f852-4f52-49ca-af4e-19bbcff3238a',\n",
       " '2049fdbd-ee2b-4135-aac4-a9c36d8ef2a4',\n",
       " 'cee1a5cd-a1fd-4a04-bfa8-11ef5896175b',\n",
       " 'a809a51d-7445-4065-94ab-7538203c9e26',\n",
       " 'd6ae8d1d-0410-4fd7-aac1-22cb6356316e',\n",
       " 'b82b1ba4-4f56-44ad-891e-95365170e529',\n",
       " 'e6be14f1-1117-4ee3-9b5f-3ec7fbac1068',\n",
       " '3f41af92-371f-44ef-a07f-186b4ea9cbc8',\n",
       " '93394baa-47d7-4858-8b49-716c6d879696',\n",
       " '4fe2383e-16da-458a-9eff-608fd4b60896',\n",
       " '41ae8b44-0a68-4e5d-8e6b-a5291abdc1d7',\n",
       " '4ebea8af-61d3-403e-a033-93c1608b8b28',\n",
       " '7d9dc8bc-8467-4f4a-b0b2-e21e671ae41b',\n",
       " '494b79e1-85a9-4cbc-a9f5-173e496a9363',\n",
       " '506c11f8-f594-4be3-8bc2-5a29319403bd',\n",
       " 'dcc9da0f-0216-4406-86ef-f3f2d1235595',\n",
       " '461763fc-05f8-4a1a-89a2-ac7aa3119437',\n",
       " 'a9a43c30-48d0-4bf5-81d5-7fc0fe401543',\n",
       " 'c3fa7bc1-419a-4674-abcb-f9dcbb3967a6',\n",
       " '1442b91f-f3bc-4de5-b4bb-514518a203d8',\n",
       " 'f9d5fcdb-9e31-4a5d-b4a6-2a22f4d77fae',\n",
       " '847a31e9-8844-4770-bed3-f814d58dd24d',\n",
       " 'e6a4fa59-812e-47f5-b877-4fd0dad7101e',\n",
       " 'ff08c14f-ebb6-4ec4-855e-bc5dcef62ae8',\n",
       " 'bab7aeba-7b08-4a55-9075-07b5ce49e0d2',\n",
       " '4a400d91-098f-4857-b881-f2f3352c5cd8',\n",
       " '1e17dd12-f5ac-4728-9fd2-aa03fa756921',\n",
       " '93848364-03af-43aa-9d0e-f4d09081406d',\n",
       " '14338d0d-c660-476e-96dc-d63365d3d9bd',\n",
       " '271882af-6e9a-4bdd-aa30-9a2e09e04994',\n",
       " '7cb62928-c0ab-4e0d-be81-f0fbdebaf083',\n",
       " 'bc9c0ad5-abb8-438f-979c-682b7a1ac69c']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(pages_chunked[:50]))]\n",
    "#uuids = [str(uuid4()) for _ in range(len(pages_chunked_cleaned1[:50]))]\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "vector_db_from_client.add_documents(documents = pages_chunked[:50], id = uuids)\n",
    "#vector_db_from_client.add_documents(documents = pages_chunked_cleaned1, id = uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.count_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.delete_collection(\"ai_model_book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Access ChromaDB and perform vector search\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Use query to perform vector search against ChromaDB vector database\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- define query\n",
    "- run vector search\n",
    "- print k=3 most similar documents\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain Query ChromaDB](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/#query-directly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "{\"error\":\"InvalidArgumentError\",\"message\":\"Expected where to have exactly one operator, got {}\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/workspaces/TeamNovaBot/.venv/lib/python3.11/site-packages/chromadb/api/fastapi.py:611\u001b[0m, in \u001b[0;36mraise_chroma_error\u001b[0;34m(resp)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 611\u001b[0m     \u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError:\n",
      "File \u001b[0;32m/workspaces/TeamNovaBot/.venv/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: http://localhost:8000/api/v1/collections/d457e486-01c9-4321-98f1-9297b76b1220/query",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m search_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTypes of Machine Learning Systems\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mvector_db_from_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearch_query\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(res\u001b[38;5;241m.\u001b[39mpage_content)\n",
      "File \u001b[0;32m/workspaces/TeamNovaBot/.venv/lib/python3.11/site-packages/langchain_chroma/vectorstores.py:582\u001b[0m, in \u001b[0;36mChroma.similarity_search\u001b[0;34m(self, query, k, filter, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    566\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    570\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run similarity search with Chroma.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;124;03m        List of documents most similar to the query text.\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 582\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m/workspaces/TeamNovaBot/.venv/lib/python3.11/site-packages/langchain_chroma/vectorstores.py:680\u001b[0m, in \u001b[0;36mChroma.similarity_search_with_score\u001b[0;34m(self, query, k, filter, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function\u001b[38;5;241m.\u001b[39membed_query(query)\n\u001b[0;32m--> 680\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__query_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere_document\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _results_to_docs_and_scores(results)\n",
      "File \u001b[0;32m/workspaces/TeamNovaBot/.venv/lib/python3.11/site-packages/langchain_core/utils/utils.py:51\u001b[0m, in \u001b[0;36mxor_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     invalid_group_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg_groups[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m invalid_groups]\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactly one argument in each of the following\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m groups must be defined:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(invalid_group_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/TeamNovaBot/.venv/lib/python3.11/site-packages/langchain_chroma/vectorstores.py:379\u001b[0m, in \u001b[0;36mChroma.__query_collection\u001b[0;34m(self, query_texts, query_embeddings, n_results, where, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;129m@xor_args\u001b[39m((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_texts\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__query_collection\u001b[39m(\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    360\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[List[Document], chromadb\u001b[38;5;241m.\u001b[39mQueryResult]:\n\u001b[1;32m    361\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Query the chroma collection.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03m    See more: https://docs.trychroma.com/reference/py-collection#query\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere_document\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/TeamNovaBot/.venv/lib/python3.11/site-packages/chromadb/api/models/Collection.py:195\u001b[0m, in \u001b[0;36mCollection.query\u001b[0;34m(self, query_embeddings, query_texts, query_images, query_uris, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    179\u001b[0m (\n\u001b[1;32m    180\u001b[0m     valid_query_embeddings,\n\u001b[1;32m    181\u001b[0m     valid_n_results,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     include,\n\u001b[1;32m    193\u001b[0m )\n\u001b[0;32m--> 195\u001b[0m query_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_query_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_n_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_where\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_where_document\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_query_response(query_results, include)\n",
      "File \u001b[0;32m/workspaces/TeamNovaBot/.venv/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/workspaces/TeamNovaBot/.venv/lib/python3.11/site-packages/chromadb/api/fastapi.py:547\u001b[0m, in \u001b[0;36mFastAPI._query\u001b[0;34m(self, collection_id, query_embeddings, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the nearest neighbors of a single embedding\"\"\"\u001b[39;00m\n\u001b[1;32m    534\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/collections/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(collection_id) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/query\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    536\u001b[0m     data\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m     ),\n\u001b[1;32m    545\u001b[0m )\n\u001b[0;32m--> 547\u001b[0m \u001b[43mraise_chroma_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m body \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(resp\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\n\u001b[1;32m    551\u001b[0m     ids\u001b[38;5;241m=\u001b[39mbody[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    552\u001b[0m     distances\u001b[38;5;241m=\u001b[39mbody\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistances\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    558\u001b[0m     included\u001b[38;5;241m=\u001b[39mbody[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincluded\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    559\u001b[0m )\n",
      "File \u001b[0;32m/workspaces/TeamNovaBot/.venv/lib/python3.11/site-packages/chromadb/api/fastapi.py:613\u001b[0m, in \u001b[0;36mraise_chroma_error\u001b[0;34m(resp)\u001b[0m\n\u001b[1;32m    611\u001b[0m     resp\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError:\n\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m(resp\u001b[38;5;241m.\u001b[39mtext))\n",
      "\u001b[0;31mException\u001b[0m: {\"error\":\"InvalidArgumentError\",\"message\":\"Expected where to have exactly one operator, got {}\"}"
     ]
    }
   ],
   "source": [
    "search_query = \"Types of Machine Learning Systems\"\n",
    "\n",
    "results = vector_db_from_client.similarity_search(\"search_query\", k = 3,)\n",
    "\n",
    "for res in results:\n",
    "    print(res.page_content)\n",
    "    print(res.metadata)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
