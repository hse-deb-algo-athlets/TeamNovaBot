{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Simple vector embedding generation\n",
    "\n",
    "**Objective:**\n",
    "Generate vector embeddings from text data.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load huggingface embedding model (`model_name=\"sentence-transformers/all-mpnet-base-v2\"`)\n",
    "- embed simple text queries\n",
    "\n",
    "How to select the right embedding model: [MTEB - Massive Text Embedding Benchmark](https://huggingface.co/blog/mteb)\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain Chroma](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/TeamNovaBot/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/workspaces/TeamNovaBot/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "embedding_model = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector length: 24\n",
      "This is a \n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test document.\"\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Perform vector search\n",
    "query_vector = text\n",
    "\n",
    "print(f\"Embedding vector length: {len(query_vector)}\")\n",
    "print(query_vector[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Generate embedding vectors with custom dataset\n",
    "\n",
    "**Objective:**\n",
    "Load custom dataset, preprocess it and generate vector embeddings.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load pdf document \"AI_Book.pdf\" via langchain document loader: `PyPDFLoader`\n",
    "- use RecursiveCharacterTextSplitter to split documents into chunks\n",
    "- generate embeddings for single documents\n",
    "\n",
    "**RecursiveCharacterTextSplitter:**\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain PyPDFLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html)\n",
    "- [Langchain RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Aurélien GéronHands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and\n",
      "TensorFlow\n",
      "Concepts, Tools, and Techniques to\n",
      "Build Intelligent SystemsSECOND EDITION\n",
      "Boston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing' metadata={'source': './AI_Book.pdf', 'page': 2}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import re\n",
    "\n",
    "pdf_doc = \"./AI_Book.pdf\"\n",
    "\n",
    "# Create pdf data loader\n",
    "# ADD HERE YOUR CODE\n",
    "loader = PyPDFLoader(\n",
    "    file_path = pdf_doc,\n",
    "    # passwort = \"my-passwort\"\n",
    "    extract_images = False,\n",
    ")\n",
    "\n",
    "# Load and split documents in chunks\n",
    "# ADD HERE YOUR CODE\n",
    "pages_chunked = RecursiveCharacterTextSplitter().split_documents(documents = loader.load())\n",
    "\n",
    "# Function to clean text by removing invalid unicode characters, including surrogate pairs\n",
    "def clean_text(text):\n",
    "    # Remove surrogate pairs\n",
    "    text = re.sub(r'[\\ud800-\\udfff]', '', text)\n",
    "    # Optionally remove non-ASCII characters (depends on your use case)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_and_create_document(chunk):\n",
    "    cleaned_text = clean_text(chunk.page_content)\n",
    "    return Document(page_content = cleaned_text, metadata = chunk.metadata)\n",
    "\n",
    "pages_chunked_cleaned1 = [clean_and_create_document(chunk) for chunk in pages_chunked]\n",
    "pages_chunked_cleaned = [clean_text(chunk.page_content) for chunk in pages_chunked]\n",
    "print(pages_chunked[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='978-1-492-03264-9\n",
      "[LSI]Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
      "by Aurélien Géron\n",
      "Copyright © 2019 Aurélien Géron. All rights reserved.\n",
      "Printed in the United States of America.\n",
      "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
      "also available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\n",
      "sales department: 800-998-9938 or corporate@oreilly.com .\n",
      "Editor:  Nicole Tache\n",
      "Interior Designer:  David FutatoCover Designer:  Karen Montgomery\n",
      "Illustrator:  Rebecca Demarest\n",
      "June 2019:  Second Edition\n",
      "Revision History for the Early Release\n",
      "2018-11-05: First Release\n",
      "2019-01-24: Second Release\n",
      "2019-03-07: Third Release\n",
      "2019-03-29: Fourth Release\n",
      "2019-04-22: Fifth Release\n",
      "See http://oreilly.com/catalog/errata.csp?isbn=9781492032649  for release details.\n",
      "The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and TensorFlow , the cover image, and related trade dress are trademarks of O’Reilly\n",
      "Media, Inc.\n",
      "While the publisher and the author have used good faith efforts to ensure that the information and\n",
      "instructions contained in this work are accurate, the publisher and the author disclaim all responsibility\n",
      "for errors or omissions, including without limitation responsibility for damages resulting from the use of\n",
      "or reliance on this work. Use of the information and instructions contained in this work is at your own\n",
      "risk. If any code samples or other technology this work contains or describes is subject to open source\n",
      "licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\n",
      "thereof complies with such licenses and/or rights.' metadata={'source': './AI_Book.pdf', 'page': 3}\n"
     ]
    }
   ],
   "source": [
    "print(pages_chunked[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 507\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of text chunks: {len(pages_chunked)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Store vector embeddings from pdf document to ChromaDB vector database.\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Store vector embeddings into ChromaDB to store knowledge.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- create chromadb client\n",
    "- create chromadb collection\n",
    "- create langchain chroma db client\n",
    "- store text document chunks and vector embeddings to vector databases\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain How To](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/#initialization-from-client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "client = chromadb.HttpClient(\n",
    "    host     = \"localhost\",\n",
    "    port     = 8000,\n",
    "    ssl      = False,\n",
    "    headers  = None,\n",
    "    settings = Settings(allow_reset = True, anonymized_telemetry = False),\n",
    "    tenant   = DEFAULT_TENANT,\n",
    "    database = DEFAULT_DATABASE,\n",
    ")\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Create a collection\n",
    "collection = client.get_or_create_collection(\"collection_name\")\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Create chromadb\n",
    "vector_db_from_client = Chroma(\n",
    "    client = client,\n",
    "    collection_name = \"collection_name\",\n",
    "    embedding_function = embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2d933ecf-281a-487f-8b78-105a28416205',\n",
       " 'de62461a-02dd-4214-991d-50d9581d0c78',\n",
       " '7a334369-c0a9-4570-8513-6f216983fb0e',\n",
       " 'cf54b9ce-7375-4d3d-914b-f1bcf4707604',\n",
       " '8da8dcd0-f626-49fd-b4fe-ec47fa5b4a01',\n",
       " 'f67f9d85-9439-4146-bfe0-079986f720ae',\n",
       " '0bdf7847-48c1-4fa0-81e8-666bb8f92943',\n",
       " 'eeb8fab0-c0bf-4268-931e-882f7fd4a47d',\n",
       " '4bb1762c-ebc1-4625-ba84-611b4faeec75',\n",
       " 'b74fd61b-e440-4f8b-91bc-cb4d94844f63',\n",
       " '7138e680-aad7-4dd5-9e98-c031175eaffa',\n",
       " '6ca21a3d-b6eb-4aac-9e0c-73bcb5a41a6b',\n",
       " '0c2b6bb0-8e78-4462-8937-1b62c6dae3bc',\n",
       " '08776c0d-8ad5-4cae-8e2d-028d4a9c4ad7',\n",
       " '53fd3b7d-0d30-49dc-ac2f-54ae5fea3c57',\n",
       " '608ce8fa-c020-41c7-b11b-09f1fa55af90',\n",
       " 'cb61c79a-afcd-4d4c-bdb3-877b54fa1ad2',\n",
       " '495e8df2-fbf4-4617-878b-84aeda41f725',\n",
       " '1c06367e-9a14-461b-a3cc-1792387bb53c',\n",
       " 'b06ff924-83dc-4309-a96a-fa5ed9bb3603',\n",
       " 'ddf3816a-cf46-4d33-80d7-4151ce9967db',\n",
       " 'bf5e8327-0b86-4bd1-895b-afc5fe65bdfd',\n",
       " 'dec31508-2edf-4fa7-a645-2d814af50e35',\n",
       " '605c5a1d-24a8-4ce3-98ad-ed3bc78f5461',\n",
       " 'e202ecfe-1ab7-4389-92df-d0b65ad96792',\n",
       " '0f3bbc6a-28b8-4f4e-8a80-ed72bbc1eac7',\n",
       " '60f1af89-2310-4733-889c-5bc47fbe3dfa',\n",
       " '0c70ce02-b9c6-4e08-8616-962f09625517',\n",
       " 'ec6511e3-8691-42ce-879e-eba8df2c0692',\n",
       " '013aeda8-71f6-4138-bde0-3bb6c4ed24ee',\n",
       " '215d55e9-d0b1-4e34-bc39-f9fc411689c4',\n",
       " '34cb0f77-1683-497e-9de9-12c03194cb14',\n",
       " 'a6ce6429-7e16-4219-b995-e10826bfbc78',\n",
       " '9aa1a436-cac3-4b5b-ba7b-797bae8e2db7',\n",
       " 'a11ca7e7-b5be-4ff6-93d4-20f8421ccce3',\n",
       " 'd982141e-d79c-4864-939f-cd4ed32c33b3',\n",
       " 'ee0bbdf6-152c-4d18-aa9f-b0983b1af34a',\n",
       " 'c41947d0-4fa7-4337-99dd-52b1096cb295',\n",
       " 'faa1db0b-4b30-4f4c-8426-40bb42b8db3e',\n",
       " 'c3e46173-ca63-4e3c-855c-e55cb50e851e',\n",
       " '13541e8a-0a13-4111-b22d-689e0018f17b',\n",
       " '17c51cbf-7317-4ecb-b286-aa7bfc7fbefd',\n",
       " '191cf355-7c82-4c9d-ade9-aa98575cd964',\n",
       " '611c1662-949f-4cef-8a89-2247029db3c6',\n",
       " 'e992cac3-5713-4aa7-8a03-e2e6a903afd7',\n",
       " '807e8f24-17d3-4f0b-8014-f31195b05680',\n",
       " '6393e5dc-8fbc-4121-9707-e924dc6d5b62',\n",
       " '36b43792-541a-4351-8669-4cab37224622',\n",
       " '8dcc0877-c2d5-41f5-af7f-5692ae02a921',\n",
       " '9c3d9952-14a6-41ec-a619-22d4976d3b72']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(pages_chunked[:50]))]\n",
    "#uuids = [str(uuid4()) for _ in range(len(pages_chunked_cleaned1[:50]))]\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "vector_db_from_client.add_documents(documents = pages_chunked[:50], id = uuids)\n",
    "#vector_db_from_client.add_documents(documents = pages_chunked_cleaned1, id = uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.count_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.delete_collection(\"ai_model_book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Access ChromaDB and perform vector search\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Use query to perform vector search against ChromaDB vector database\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- define query\n",
    "- run vector search\n",
    "- print k=3 most similar documents\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain Query ChromaDB](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/#query-directly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame the Problem                                                                                                    39\n",
      "Select a Performance Measure                                                                                  42\n",
      "Check the Assumptions                                                                                             45\n",
      "Get the Data                                                                                                                    45\n",
      "Create the Workspace                                                                                                45\n",
      "Download the Data                                                                                                    49\n",
      "Take a Quick Look at the Data Structure                                                                50\n",
      "Create a Test Set                                                                                                          54\n",
      "Discover and Visualize the Data to Gain Insights                                                     58\n",
      "Visualizing Geographical Data                                                                                 59\n",
      "Looking for Correlations                                                                                           62\n",
      "Experimenting with Attribute Combinations                                                        65\n",
      "Prepare the Data for Machine Learning Algorithms                                                66\n",
      "Data Cleaning                                                                                                             67\n",
      "Handling Text and Categorical Attributes                                                              69\n",
      "Custom Transformers                                                                                                71\n",
      "Feature Scaling                                                                                                            72\n",
      "Transformation Pipelines                                                                                          73\n",
      "Select and Train a Model                                                                                               75\n",
      "Training and Evaluating on the Training Set                                                         75\n",
      "Better Evaluation Using Cross-Validation                                                              76\n",
      "Fine-Tune Y our Model                                                                                                  79\n",
      "Grid Search                                                                                                                 79\n",
      "Randomized Search                                                                                                   81\n",
      "Ensemble Methods                                                                                                     82\n",
      "Analyze the Best Models and Their Errors                                                             82\n",
      "Evaluate Y our System on the Test Set                                                                      83\n",
      "Launch, Monitor, and Maintain Y our System                                                            84\n",
      "Try It Out!                                                                                                                       85\n",
      "Exercises                                                                                                                          85\n",
      "3.Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  87\n",
      "MNIST                                                                                                                             87\n",
      "Training a Binary Classifier                                                                                          90\n",
      "Performance Measures                                                                                                  90\n",
      "{'page': 5, 'source': './AI_Book.pdf'}\n",
      "\n",
      "\n",
      "CHAPTER 1\n",
      "The Machine Learning Landscape\n",
      "With Early Release ebooks, you get books in their earliest form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can take advantage of these technologies long before the official\n",
      "release of these titles. The following will be Chapter 1 in the final\n",
      "release of the book.\n",
      "When most people hear “Machine Learning, ” they picture a robot: a dependable but‐\n",
      "ler or a deadly Terminator depending on who you ask. But Machine Learning is not\n",
      "just a futuristic fantasy, it’s already here. In fact, it has been around for decades in\n",
      "some specialized applications, such as Optical Character Recognition  (OCR). But the\n",
      "first ML application that really became mainstream, improving the lives of hundreds\n",
      "of millions of people, took over the world back in the 1990s: it was the spam filter .\n",
      "Not exactly a self-aware Skynet, but it does technically qualify as Machine Learning\n",
      "(it has actually learned so well that you seldom need to flag an email as spam any‐\n",
      "more). It was followed by hundreds of ML applications that now quietly power hun‐\n",
      "dreds of products and features that you use regularly, from better recommendations\n",
      "to voice search.\n",
      "Where does Machine Learning start and where does it end? What exactly does it\n",
      "mean for a machine to learn  something? If I download a copy of Wikipedia, has my\n",
      "computer really “learned” something? Is it suddenly smarter? In this chapter we will\n",
      "start by clarifying what Machine Learning is and why you may want to use it.\n",
      "Then, before we set out to explore the Machine Learning continent, we will take a\n",
      "look at the map and learn about the main regions and the most notable landmarks:\n",
      "supervised versus unsupervised learning, online versus batch learning, instance-\n",
      "based versus model-based learning. Then we will look at the workflow of a typical ML\n",
      "project, discuss the main challenges you may face, and cover how to evaluate and\n",
      "fine-tune a Machine Learning system.\n",
      "3\n",
      "{'page': 28, 'source': './AI_Book.pdf'}\n",
      "\n",
      "\n",
      "Figure 1-18. A few possible linear models\n",
      "Before you can use your model, you need to define the parameter values θ0 and θ1.\n",
      "How can you know which values will make your model perform best? To answer this\n",
      "question, you need to specify a performance measure. Y ou can either define a utility\n",
      "function  (or fitness  function ) that measures how good  your model is, or you can define\n",
      "a cost function  that measures how bad it is. For linear regression problems, people\n",
      "typically use a cost function that measures the distance between the linear model’s\n",
      "predictions and the training examples; the objective is to minimize this distance.\n",
      "This is where the Linear Regression algorithm comes in: you feed it your training\n",
      "examples and it finds the parameters that make the linear model fit best to your data.\n",
      "This is called training  the model. In our case the algorithm finds that the optimal\n",
      "parameter values are θ0 = 4.85 and θ1 = 4.91 × 10–5.\n",
      "Now the model fits the training data as closely as possible (for a linear model), as you\n",
      "can see in Figure 1-19 .\n",
      "Figure 1-19. The linear model that fits the training data best\n",
      "Types of Machine Learning Systems | 21\n",
      "{'page': 46, 'source': './AI_Book.pdf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Types of Machine Learning Systems\"\n",
    "\n",
    "results = vector_db_from_client.similarity_search(\"search_query\", k = 3,)\n",
    "\n",
    "for res in results:\n",
    "    print(res.page_content)\n",
    "    print(res.metadata)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
